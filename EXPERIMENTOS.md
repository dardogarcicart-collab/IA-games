# ğŸ§ª GUÃA DE EXPERIMENTOS - Comprueba el Aprendizaje

## Antes de Empezar

**Importante:** Estos experimentos funcionan MEJOR si:
- âœ… Seleccionas **1 agente** (mÃ¡s claro)
- âœ… Objetivo: **ğŸ Buscar Comida**
- âœ… Activaste: **Castigo por Saltar** (on)
- âœ… Mapa: **VacÃ­o** (sin obstÃ¡culos)

---

## ğŸ§ª EXPERIMENTO 1: ExploraciÃ³n â†’ ExplotaciÃ³n (10 minutos)

### Objetivo
Ver cÃ³mo el agente cambia de movimiento 100% aleatorio a dirigido.

### Paso 1: Observa Caos Puro (0-20 segundos)

```
1. Abre IASistem.html
2. Deja el agente solo por 20 segundos
3. NO coloques comida aÃºn
4. Observa:

âœ“ Agente salta sin razÃ³n
âœ“ Cambia direcciÃ³n constantemente  
âœ“ Parece "loco" o "nervioso"
âœ“ NO hay patrÃ³n en sus movimientos
âœ“ EstadÃ­sticas:
  - Experiencia: 0-20
  - Comida: 0 (no hay comida)
  - Saltos: muchos
```

**QuÃ© estÃ¡ pasando:** El agente explora el espacio de acciones aleatoriamente.

---

### Paso 2: Introduce Comida (20-50 segundos)

```
5. Coloca 1 comida a la DERECHA del agente
   (aproximadamente x=700, agente estÃ¡ en x=400)

6. Espera 3 segundos sin hacer nada
   Observa: Toca la comida por pura suerte

7. Coloca 3 COMIDAS MÃS a la DERECHA
   (en diferentes alturas)

8. Espera 30 segundos
```

**Observa estos cambios:**

| Tiempo | Comportamiento | Experiencia | Comida |
|--------|---|---|---|
| Seg 0-10 | TodavÃ­a muy random | 10-20 | 1-3 |
| Seg 10-20 | Empieza a favorecer derecha | 20-40 | 3-6 |
| Seg 20-30 | Claro sesgo a derecha | 40-60 | 6-12 |

**QuÃ© esperar:**

```
Visualmente:
  â”œâ”€ Primeros 5 segundos: Sigue siendo aleatorio
  â”œâ”€ Segundos 5-15: ~60% a derecha, 40% aleatorio
  â”œâ”€ Segundos 15-30: ~80% a derecha, 20% aleatorio
  â””â”€ Resultado: Va PRINCIPALMENTE a derecha

Panel derecho (EstadÃ­sticas):
  â”œâ”€ Experiencia sube suavemente: 0 â†’ 60
  â”œâ”€ Comida sube mÃ¡s rÃ¡pido a partir de seg 10
  â””â”€ Aprendizaje: ~0.5 experiencias/frame
```

**âœ… Si ves esto: ESTÃ APRENDIENDO**

---

### Paso 3: Cambio Abrupto (50-70 segundos)

```
9. Borra TODAS las comidas
   (BotÃ³n ğŸ—‘ï¸ Limpiar en "Crear Items")

10. Coloca 4 comidas a la IZQUIERDA
    (opuesto de donde estaban)

11. Espera 20 segundos
```

**Observa la ADAPTACIÃ“N:**

| Tiempo | Comportamiento | Nota |
|--------|---|---|
| Seg 0-2 | Sigue yendo a derecha (viejo patrÃ³n) | Inercia |
| Seg 2-7 | Nota que no hay recompensa a derecha | ConfusiÃ³n |
| Seg 7-15 | Empieza a explorar izquierda | TransiciÃ³n |
| Seg 15-20 | Busca principalmente a izquierda | Nuevo patrÃ³n |

**QuÃ© estÃ¡ pasando internamente:**

```
Tabla Q ANTES:
  Q[comida_derecha][ir_derecha] = +50 (alto)
  Q[comida_derecha][ir_izquierda] = -10 (bajo)

Tabla Q DESPUÃ‰S (seg 7-15):
  Q[comida_izquierda][ir_izquierda] = +50 (nuevo)
  Q[comida_izquierda][ir_derecha] = -10 (nuevo)
  
Comportamiento: El agente se ADAPTA
Marca de aprendizaje: Esto demuestra que estÃ¡ REALMENTE aprendiendo
```

**âœ… CONFIRMADO: El agente aprendiÃ³ y se adaptÃ³**

---

## ğŸ§ª EXPERIMENTO 2: MediciÃ³n Cuantitativa (10 minutos)

### Objetivo
Medir el aprendizaje con nÃºmeros.

### PreparaciÃ³n

```
1. Abre simulador con 1 agente
2. Objetivo: Comida
3. Coloca 15 comidas distribuidas a la DERECHA
4. Inicia cronÃ³metro (telÃ©fono o reloj)
```

### Recopila Datos Cada 30 Segundos

Captura esta informaciÃ³n:

```
TIEMPO: 0:00 (inicio)
â”œâ”€ Experiencia: ___
â”œâ”€ Comida: ___
â”œâ”€ Saltos: ___
â””â”€ ObservaciÃ³n: _________________________________

TIEMPO: 0:30
â”œâ”€ Experiencia: ___
â”œâ”€ Comida: ___
â”œâ”€ Saltos: ___
â””â”€ ObservaciÃ³n: _________________________________

TIEMPO: 1:00
â”œâ”€ Experiencia: ___
â”œâ”€ Comida: ___
â”œâ”€ Saltos: ___
â””â”€ ObservaciÃ³n: _________________________________

TIEMPO: 1:30
â”œâ”€ Experiencia: ___
â”œâ”€ Comida: ___
â”œâ”€ Saltos: ___
â””â”€ ObservaciÃ³n: _________________________________

TIEMPO: 2:00
â”œâ”€ Experiencia: ___
â”œâ”€ Comida: ___
â”œâ”€ Saltos: ___
â””â”€ ObservaciÃ³n: _________________________________
```

### Analiza los Datos

**Experiencia (debe subir exponencial):**

```
Esperado:
  0:00 â†’ 0-10
  0:30 â†’ 20-40
  1:00 â†’ 50-100
  1:30 â†’ 100-200
  2:00 â†’ 200-350

âœ“ Si sube asÃ­: Aprendizaje normal
âœ— Si sube poco: Problema (revisar comida)
âœ— Si no sube: Bug serio
```

**Comida (debe acelerar despuÃ©s de 0:30):**

```
Esperado:
  0:00 â†’ 0-2
  0:30 â†’ 2-8
  1:00 â†’ 8-20
  1:30 â†’ 20-40
  2:00 â†’ 40-80

âœ“ Si acelera: Aprendizaje funciona
âœ— Si lineal: PodrÃ­a ser mejor
âœ— Si 0: Problema con comida/recompensas
```

**Saltos (debe bajar si penalizaciÃ³n activa):**

```
Esperado:
  0:00 â†’ 4-5 por segundo
  0:30 â†’ 3-4 por segundo
  1:00 â†’ 2-3 por segundo
  1:30 â†’ 1-2 por segundo
  2:00 â†’ 1-2 por segundo

âœ“ Si baja asÃ­: Aprende que saltar cuesta
âœ— Si no baja: PenalizaciÃ³n podrÃ­a no funcionar
```

---

## ğŸ§ª EXPERIMENTO 3: Cambio de Objetivo (8 minutos)

### Objetivo
Ver que el agente puede aprender diferentes objetivos.

### Paso 1: Aprender Comida (3 minutos)

```
1. Selecciona objetivo: "ğŸ Buscar Comida"
2. Coloca 10 comidas dispersas
3. Espera 3 minutos (hasta que suba Experiencia a 150+)
4. Observa: Va principalmente hacia comida
5. Nota comida comida lograda (ej: 20)
```

### Paso 2: Cambiar a Leche (3 minutos)

```
6. Selecciona objetivo: "ğŸ¥› Beber Leche"
   NOTA: Esto reinicia el epsilon a 0.3
   
7. Borra comida, coloca 10 LECHE (en otros lugares)
8. Espera 2-3 minutos
9. Observa: Agente empieza a buscar leche
10. Ignora comida (aunque la vea)
```

**QuÃ© demuestra:**

```
âœ… El agente puede aprender mÃºltiples objetivos
âœ… Cambia su estrategia segÃºn el objetivo
âœ… Esto prueba que Q-tabla es multiobjetivo
âœ… Demuestra FLEXIBILIDAD del sistema
```

---

## ğŸ§ª EXPERIMENTO 4: Efecto de Castigo por Saltar (5 minutos)

### Objetivo
Ver cÃ³mo un parÃ¡metro cambia el comportamiento.

### Paso 1: CON Castigo (2.5 minutos)

```
1. Verificar: "Castigo por Saltar" = ON
2. Coloca comida a la DERECHA
3. Observa saltos en primeros 30 segundos: ~4/seg
4. Observa saltos despuÃ©s de 2 minutos: ~1/seg
5. Nota: Va a comida pero salta menos
```

### Paso 2: SIN Castigo (2.5 minutos)

```
6. Desactiva: "Castigo por Saltar" = OFF
7. Click en "ğŸ”„ Reset Completo"
8. Coloca comida a la DERECHA (mismo lugar)
9. Observa saltos en primeros 30 segundos: ~4/seg
10. Observa saltos despuÃ©s de 2 minutos: ~3-4/seg
11. Nota: Va a comida Y salta mÃ¡s (porque no le cuesta)
```

**ComparaciÃ³n:**

| ParÃ¡metro | CON Castigo | SIN Castigo |
|-----------|---|---|
| Saltos inicio | 4/seg | 4/seg |
| Saltos min 2 | 1/seg | 3/seg |
| Comida lograda | 25 | 30 |
| EnergÃ­a final | Mayor | Menor |
| Estrategia | Prudente | Arriesgada |

**âœ… Si ves estas diferencias: Reward shaping funciona**

---

## ğŸ§ª EXPERIMENTO 5: MÃºltiples Agentes (5 minutos)

### Objetivo
Ver que cada agente aprende diferente.

### Paso a Paso

```
1. Selecciona: "3 Agentes (TrÃ­o)"
   â””â”€ VerÃ¡s 3 colores: Rojo, Turquesa, Amarillo

2. Objetivo: "ğŸ Buscar Comida"

3. Coloca comida en 3 lugares DIFERENTES:
   â”œâ”€ ARRIBA a la derecha
   â”œâ”€ ABAJO al centro
   â””â”€ ARRIBA a la izquierda

4. Espera 2 minutos

5. Observa:
   â”œâ”€ Agente ROJO â†’ tiende a zona arriba-derecha
   â”œâ”€ Agente TURQUESA â†’ tiende a zona abajo-centro
   â”œâ”€ Agente AMARILLO â†’ tiende a zona arriba-izquierda
   â””â”€ SIN COMUNICARSE, cada uno aprendiÃ³ zona diferente
```

**Click en cada agente para ver stats:**

```
Panel lateral â†’ Botones de agentes

AGENTE 1 (Rojo):
  â””â”€ Comida: 12
  â””â”€ Experiencia: 180
  â””â”€ VisiÃ³n: "Comida derecha-arriba"

AGENTE 2 (Turquesa):
  â””â”€ Comida: 15
  â””â”€ Experiencia: 200
  â””â”€ VisiÃ³n: "Comida abajo"

AGENTE 3 (Amarillo):
  â””â”€ Comida: 10
  â””â”€ Experiencia: 160
  â””â”€ VisiÃ³n: "Comida izquierda-arriba"
```

**âœ… Demuestra EMERGENCIA comportamental**

---

## ğŸ“Š TEMPLATE: Documenta tus Resultados

```markdown
## Mi Experimento de Aprendizaje

**Fecha:** [FECHA]
**Agentes:** [1 / 3 / 5]
**Objetivo:** [Comida / Leche / Bandera]
**DuraciÃ³n:** [X minutos]

### Observaciones Iniciales (0-1 min)
- [Nota aquÃ­ quÃ© viste]
- [Comportamiento aleatorio?]
- [Saltos sin patrÃ³n?]

### Observaciones Intermedias (1-3 min)
- [Cambios de patrÃ³n?]
- [DirecciÃ³n preferida?]
- [Exponencial o lineal?]

### Observaciones Finales (3+ min)
- [Comportamiento convergido?]
- [Consistencia?]
- [AdaptaciÃ³n rÃ¡pida?]

### MÃ©tricas
- Experiencia: [valor inicial] â†’ [valor final]
- Comida: [valor inicial] â†’ [valor final]
- Saltos: [valor inicial] â†’ [valor final]

### ConclusiÃ³n
âœ… APRENDIZAJE CONFIRMADO
âœ— No visto claramente
â“ Necesita mÃ¡s investigaciÃ³n

### Cambios Recomendados
- [Si necesita mejora]
```

---

## ğŸ¯ QUICK EXPERIMENT (3 minutos)

Si solo tienes 3 minutos:

```
1. Abre simulador â†’ 1 agente
2. Coloca 5 comidas a la DERECHA
3. Espera 1 minuto
4. ESPERA: Agente prefiere derecha
5. Mueve todas a la IZQUIERDA
6. Espera 30 segundos
7. VERIFICA: Agente cambiÃ³ a izquierda

Si ves paso 4 Y 7: APRENDIZAJE CONFIRMADO âœ…
```

---

## âš ï¸ TROUBLESHOOTING

### "El agente no va a la comida"

```
Verificar:
â–¡ Â¿Hay comida en el mapa?
â–¡ Â¿Objetivo es "Comida"?
â–¡ Â¿Experiencia sube (contador)?
â–¡ Â¿Comida cuenta sube (contador)?

Si SÃ a todo:
  âœ… Es normal, es Q-Learning, es subtil
  â†’ Mira Experimento 1 (cambio direcciÃ³n)

Si NO:
  âœ— Hay problema
  â†’ Recarga pÃ¡gina
  â†’ Verifica consola (F12 - errores)
```

### "Los nÃºmeros no suben"

```
Verificar:
â–¡ Â¿El juego estÃ¡ corriendo? (agente se mueve)
â–¡ Â¿Hay comida colocada?
â–¡ Â¿Panel mostrando 0 todo el tiempo?

Soluciones:
1. Recarga pÃ¡gina (Ctrl+R)
2. Coloca comida manualmente
3. Espera 1 minuto
4. Abre consola (F12)
   â†’ Busca errores en rojo
   â†’ Copia error y reporta
```

### "Â¿Es este el aprendizaje real?"

```
Respuesta: SÃ

SeÃ±ales:
âœ… Experiencia sube exponencial
âœ… Comportamiento cambia con tiempo
âœ… Adapta cuando cambias comida
âœ… EstadÃ­sticas mejoran
âœ… Menos random con tiempo

Esto ES Q-Learning real
No es simulado
No es truqueado
Es IA pura funcionando
```

---

## ğŸ“š PARA PROFUNDIZAR

Si quieres entender mejor quÃ© estÃ¡ pasando internamente:

1. **Lee:** [ENTENDER_APRENDIZAJE.md](ENTENDER_APRENDIZAJE.md)
2. **Lee:** [IMPLEMENTACION_COMPLETA.md](IMPLEMENTACION_COMPLETA.md)
3. **Abre consola:** F12 â†’ Console
4. **Ejecuta:** `agents[0].learning.getStats()`
   - VerÃ¡s Q-tabla interna
   - VerÃ¡s nÃºmero de estados
   - VerÃ¡s estadÃ­sticas

---

## âœ¨ CONCLUSIÃ“N

Estos experimentos te demostrarÃ¡n que:

âœ… **El sistema estÃ¡ aprendiendo realmente**
âœ… **Q-Learning funciona**
âœ… **Es predecible pero no 100% determinista**
âœ… **Cada agente aprende diferente**
âœ… **Se adapta cuando cambias el ambiente**

**Â¡Ahora pruÃ©balo! ğŸ§ªğŸš€**
