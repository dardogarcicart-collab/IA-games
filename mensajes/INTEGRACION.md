# ğŸ§  GuÃ­a de IntegraciÃ³n de MÃ³dulos

## Resumen de la Arquitectura

Has creado una IA modular que funciona como un verdadero sistema cognitivo. AquÃ­ te explico cÃ³mo **PIENSAN** los 3 sistemas juntos.

## ğŸ§  + ğŸ“š + âš™ï¸ = ğŸ¤– IA REAL

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AGENT (Agente Principal)                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   BRAIN          â”‚  â”‚   LEARNING       â”‚  â”‚   PHYSICS  â”‚  â”‚
â”‚  â”‚  (Cognitivo)     â”‚  â”‚  (Aprendizaje)   â”‚  â”‚  (Motor)   â”‚  â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
â”‚  â”‚ â€¢ PercepciÃ³n     â”‚  â”‚ â€¢ Q-Learning     â”‚  â”‚ â€¢ Gravedad â”‚  â”‚
â”‚  â”‚ â€¢ Emociones      â”‚  â”‚ â€¢ Tabla Q        â”‚  â”‚ â€¢ Colisionesâ”‚ â”‚
â”‚  â”‚ â€¢ PredicciÃ³n     â”‚  â”‚ â€¢ Experiencias   â”‚  â”‚ â€¢ Metabolismo
â”‚  â”‚ â€¢ DecisiÃ³n       â”‚  â”‚ â€¢ Patrones       â”‚  â”‚ â€¢ BiomecÃ¡nica
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚         â”‚                      â”‚                     â”‚         â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                                â”‚                               â”‚
â”‚                          â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”                        â”‚
â”‚                          â”‚   UPDATE   â”‚                        â”‚
â”‚                          â”‚  (60 FPS)  â”‚                        â”‚
â”‚                          â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚   RENDERIZADO  â”‚
                         â”‚   (Canvas)     â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”„ Flujo por Frame (Ciclo de ActualizaciÃ³n)

### Frame N

```
1ï¸âƒ£  PERCEPCIÃ“N (BRAIN)
    â”œâ”€ brain.perceiveEnvironment(foods, blocks)
    â”œâ”€ Sensores: VisiÃ³n, PropiocepciÃ³n, InterocepciÃ³n
    â”œâ”€ Actualiza estado sensorial interno
    â””â”€ Busca comida y bloques mÃ¡s cercanos

2ï¸âƒ£  OBTENER ESTADO (LEARNING)
    â”œâ”€ state = learning.getState(agent, foods, blocks)
    â”œâ”€ Ej: "right_near_low_ground"
    â”œâ”€ Discretiza el espacio continuo
    â””â”€ Identifica patrÃ³n del mundo actual

3ï¸âƒ£  DECISIÃ“N INTELIGENTE (BRAIN + LEARNING)
    â”œâ”€ qValues = learning.getQValues(state)
    â”œâ”€ decision = brain.makeDecision(state, qValues, epsilon)
    â”œâ”€ Considera: urgencia, emociones, patrones aprendidos
    â””â”€ Retorna acciÃ³n: left, right, jump, idle

4ï¸âƒ£  APRENDER DEL PASADO (LEARNING)
    â”œâ”€ Si frame anterior existe:
    â”œâ”€ Calcula recompensa r (comida, energÃ­a, etc)
    â”œâ”€ learning.recordExperience(lastState, lastAction, r, state)
    â”œâ”€ Actualiza Q-values con Bellman
    â””â”€ Tabla Q mejora gradualmente

5ï¸âƒ£  EJECUTAR ACCIÃ“N (BRAIN + PHYSICS)
    â”œâ”€ agent.executeAction(decision.action)
    â”œâ”€ Llama a physics.applyMovement()
    â”œâ”€ O physics.applyJump()
    â””â”€ Modifica velocidad del agente

6ï¸âƒ£  FÃSICA (PHYSICS)
    â”œâ”€ physics.applyGravity(agent)
    â”œâ”€ physics.applyFriction(agent)
    â”œâ”€ physics.updatePosition(agent)
    â”œâ”€ physics.applyWorldBoundaries(agent)
    â”œâ”€ physics.resolveBlockCollisions(agent, blocks)
    â””â”€ Actualiza posiciÃ³n y velocidad realista

7ï¸âƒ£  COLISIONES CON COMIDA (BRAIN + LEARNING)
    â”œâ”€ agent.checkFoodCollisions(foods)
    â”œâ”€ Si colisiona:
    â”œâ”€ reward = +50 (+ bonus si energÃ­a baja)
    â”œâ”€ learning.recordExperience(..., reward, ...)
    â”œâ”€ brain.emotionDecay baja (menos frustraciÃ³n)
    â””â”€ foodEaten++

8ï¸âƒ£  METABOLISMO (PHYSICS)
    â”œâ”€ cost = physics.calculateMetabolicCost(agent)
    â”œâ”€ Considera: reposo, movimiento, salto, fatiga
    â”œâ”€ agent.energy -= cost
    â””â”€ Fatiga se recupera (biomecÃ¡nica)

9ï¸âƒ£  EMOCIONES (BRAIN)
    â”œâ”€ brain.processEmotions()
    â”œâ”€ Ajusta hormonas segÃºn circunstancias
    â”œâ”€ agent.updateMood()
    â””â”€ ExpresiÃ³n facial cambia (neutral/angry/tired)

ğŸ”Ÿ  OPTIMIZACIÃ“N (LEARNING)
    â”œâ”€ learning.updateEpsilon()
    â”œâ”€ Epsilon *= 0.9998 (decay gradual)
    â””â”€ Menos exploraciÃ³n aleatoria con el tiempo

1ï¸âƒ£1ï¸âƒ£  RENDERIZADO (UI)
    â”œâ”€ Dibujar mundo (cielo, suelo, etc)
    â”œâ”€ Dibujar bloques
    â”œâ”€ Dibujar comida
    â”œâ”€ Dibujar agente con expresiÃ³n
    â””â”€ Actualizar UI (energÃ­a, mood, stats)
```

## ğŸ’¡ Ejemplo PrÃ¡ctico: El Agente Aprende

### Minuto 0:0 (Inicio)

```
Estado: "none_far_high_ground"
AcciÃ³n: ALEATORIO â†’ "right" (epsilon=0.40)
Resultado: Se mueve despacho a la derecha
Recompensa: -0.05 (penalizaciÃ³n pasiva)
Q-table: Q[none_far_high_ground][right] = -0.05
```

### Minuto 0:15 (Encuentro comida)

```
Crear comida â†’ Estado cambia a "right_near_high_ground"
AcciÃ³n: ALEATORIO â†’ "idle" (epsilon=0.39)
ColisiÃ³n: Â¡Come comida!
Recompensa: +75 (50 base + 25 bonus porque energÃ­a baja)
Q-table: Q[right_near_high_ground][idle] = 75
EmociÃ³n: dopamina â†‘, satisfaction â†‘, enojado baja
```

### Minuto 1:00 (PatrÃ³n se forma)

```
Estado: "left_far_medium_ground"
AcciÃ³n: Epsilon-greedy
- 36% EXPLORACIÃ“N aleatoria (epsilon=0.36)
- 64% EXPLOTACIÃ“N: "right" (Q-value mÃ¡s alto)
```

### Minuto 5:00 (Convergencia)

```
Estado: "right_far_low_ground"
AcciÃ³n: Casi siempre "right" 
- 5% EXPLORACIÃ“N (min epsilon)
- 95% EXPLOTACIÃ“N
Comportamiento: Definitivamente busca comida hacia donde la vea
```

## ğŸ“Š Variables que Cambian DinÃ¡micamente

### En BRAIN (Inteligencia)

```javascript
mentalState = {
    attention: 1.0,        // â†‘ cuando ve comida
    urgency: 0.8,          // â†‘ cuando energÃ­a baja
    stress: 0.3,           // â†‘ cuando falla
    motivation: 0.6        // â†‘ con dopamina
}

hormones = {
    adrenaline: 0.2,       // â†‘ peligro/urgencia
    dopamine: 0.8,         // â†‘ despuÃ©s de comer
    cortisol: 0.1,         // â†‘ estrÃ©s
    ghrelin: 0.4           // â†‘ hambre
}
```

### En LEARNING (Aprendizaje)

```javascript
qTable = {
    "left_far_high_ground": { 
        left: 5.2, 
        right: 2.1, 
        jump: -0.5, 
        idle: 0 
    },
    "right_near_low_ground": { 
        left: -1.0, 
        right: 12.5,  // â† MÃ¡s alto = mejor acciÃ³n
        jump: 3.2, 
        idle: 8.1 
    }
}

epsilon: 0.25  // Disminuye cada frame
convergence: 0.73  // % de estabilidad
```

### En PHYSICS (Motor)

```javascript
biomechanics = {
    muscularFatigue: 0.3,   // â†‘ con saltos
    lactateLevel: 0.1,      // â†‘ movimiento intenso
    oxygenDebt: 0.0,        // â†‘ anaerÃ³bico
    muscleTemperature: 37.2 // â†‘ con esfuerzo
}

metabolicCost = 1.5  // energÃ­a/frame
```

## ğŸ¯ CÃ³mo Se Comunican los MÃ³dulos

### BRAIN â†’ LEARNING

```
brain.makeDecision() 
  â†“
  Usa learning.getQValues() para ver quÃ© aprendiÃ³
  â†“
  Usa learning.epsilon para decidir: explorar o explotar
  â†“
  Retorna decision con action + reasoning
```

### BRAIN â†’ PHYSICS

```
agent.executeAction(action)
  â†“
  Llama a physics.applyMovement(agent, direction)
  o physics.applyJump(agent, jumpPenalty)
  â†“
  Physics actualiza agent.vx, agent.vy, agent.vy
  â†“
  Next frame: PHYSICS controla gravedad y colisiones
```

### LEARNING â†’ AGENT

```
learning.recordExperience(state, action, reward, nextState)
  â†“
  Actualiza qTable[state][action]
  â†“
  Calcula nuevas preferencias
  â†“
  PrÃ³ximas decisiones serÃ¡n diferentes
```

### PHYSICS â†’ BRAIN

```
energy = physics.calculateMetabolicCost()
  â†“
  agent.energy -= cost
  â†“
  brain.perceiveEnvironment() detecta energy baja
  â†“
  brain.processEmotions() â†’ mood = 'tired'
  â†“
  ExpresiÃ³n facial cambia
```

## ğŸ§¬ Tabla Q como "Memoria Inteligente"

La tabla Q es bÃ¡sicamente:
- **Clave**: DescripciÃ³n del mundo (estado)
- **Valor**: QuÃ© tan buena es cada acciÃ³n en ese estado

```
"left_near_high_ground" â†’ {
    left:  -5.2  â† MALA (se aleja de comida)
    right: 25.8  â† EXCELENTE (hacia comida)
    jump:  8.3   â† OK (saltar hacia comida)
    idle:  -0.1  â† MALA (pierde oportunidad)
}
```

Con el tiempo:
- Los valores **correctos suben** (valores Q positivos altos)
- Los valores **incorrectos bajan** (valores Q negativos)
- El agente **elige automÃ¡ticamente** la acciÃ³n de mayor valor

## ğŸ“ El Aprendizaje en FÃ³rmula

Cada vez que el agente toma una acciÃ³n y ve resultado:

```
Î”Q = Î± [r + Î³ max(Q(s')) - Q(s,a)]
      â†“   â†‘       â†‘              â†‘
    cambio recompensa futuro valor actual
```

- **Î± (learning rate) = 0.15**: QuÃ© tan rÃ¡pido cambia
- **Î³ (discount) = 0.95**: CuÃ¡nto importa el futuro
- **r (reward)**: Lo que obtuvo en este frame
- **Q(s')**: Lo mejor que espera el prÃ³ximo estado

## ğŸ”¬ Ejemplo NumÃ©rico

### Inicial
```
Q[right_far][right] = 0
```

### Frame 1: Se mueve a la derecha
```
State: right_far
Action: right
Reward: -0.05 (penalizaciÃ³n pasiva)
NextState: right_near (Â¡mÃ¡s cerca!)
MaxQ(right_near) = 12.5

Î”Q = 0.15 * (-0.05 + 0.95 * 12.5 - 0)
   = 0.15 * (11.825)
   = 1.77

Q[right_far][right] = 0 + 1.77 = 1.77
```

### Frame 2: Come comida
```
State: right_near
Action: idle
Reward: +75 (Â¡comida!)
NextState: right_far (nuevo ciclo)
MaxQ(right_far) = 1.77

Î”Q = 0.15 * (75 + 0.95 * 1.77 - 0)
   = 0.15 * (76.68)
   = 11.5

Q[right_near][idle] = 0 + 11.5 = 11.5
```

Â¡Ahora sabe que "estar cerca" â†’ "idle" = bueno!

## ğŸ® CÃ³mo Experimentar

### Para ver BRAIN trabajando:
1. Crea 1 manzana en el centro
2. Observa el patrÃ³n de bÃºsqueda
3. Cambia su expresiÃ³n segÃºn emociones

### Para ver LEARNING trabajando:
1. Coloca muchas manzanas aleatorias
2. Observa cÃ³mo mejora el contador de "Experiencia"
3. VerÃ¡s cÃ³mo Q-values convergen

### Para ver PHYSICS trabajando:
1. Crea bloques obstaculizando el camino
2. Activa/desactiva "Castigo por Saltar"
3. Observa cÃ³mo aprende a saltarlos (o no)

---

**La belleza de este sistema:** Ninguno de los 3 mÃ³dulos sabe completamente quÃ© hacen los otros, pero juntos **crean inteligencia real**. ğŸ¤–âœ¨
