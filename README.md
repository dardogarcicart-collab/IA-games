# ğŸ¤– Simulador de Vida Artificial Avanzado - Q-Learning Real

Un simulador completamente funcional de IA que **aprende de verdad** usando Q-Learning en tiempo real.

## ğŸ¯ OBJETIVOS QUE PUEDE APRENDER

```
ğŸ Buscar Comida      â†’ Localiza y come automÃ¡ticamente
ğŸ¥› Beber Leche        â†’ Prioriza leche sobre comida
ğŸš© Alcanzar Bandera   â†’ Navega a objetivos especÃ­ficos
ğŸš« No Saltar          â†’ Aprende que saltar cuesta energÃ­a
```

## ğŸ› ï¸ ELEMENTOS DEL MUNDO

| Elemento | Efecto | EnseÃ±anza |
|----------|--------|-----------|
| ğŸ Comida | +35 energÃ­a | Recompensa: +50 |
| ğŸ¥› Leche | +50 energÃ­a | Recompensa: +40 |
| ğŸ§± Bloque | ObstÃ¡culo | No pasar |
| âš ï¸ Pincho | -20 energÃ­a | Evitar siempre |
| ğŸš© Bandera | Objetivo | Recompensa: +200 |

## ğŸ§  CÃ“MO FUNCIONA LA IA

### 1. **PERCEPCIÃ“N INTELIGENTE** ğŸ”
- Escanea 100 pÃ­xeles alrededor
- Detecta comida, leche, pinchos, banderas
- Sabe direcciÃ³n y distancia exacta
- Identifica obstÃ¡culos inmediatos

### 2. **DECISIÃ“N NATURAL** ğŸ’­
```
âœ“ No spammea izq/derecha (cooldown 3 frames)
âœ“ EVITA PINCHOS SIEMPRE (mÃ¡xima prioridad)
âœ“ Aprende costo del salto (promedia Ãºltimos 50)
âœ“ Elige acciones deliberadamente
```

### 3. **APRENDIZAJE REAL** ğŸ§¬
```
Q(s,a) = Q(s,a) + Î±[r + Î³Ã—max(Q(s',a')) - Q(s,a)]

Î± (learning rate) = 0.2      (velocidad)
Î³ (discount factor) = 0.95   (valor futuro)
Îµ (epsilon) = 0.5 â†’ 0.05    (exploraciÃ³n)
```

### 4. **DINÃMICAS ADAPTATIVAS** âš¡
- Prioridad 1: Evitar pinchos â†’ saltar
- Prioridad 2: Si saltar es caro â†’ no saltar
- Prioridad 3: Epsilon-greedy (explorar vs usar lo aprendido)
- Prioridad 4: Mejor acciÃ³n conocida

## ğŸ“Š ESTADÃSTICAS EN TIEMPO REAL

- **Estado**: Dying ğŸ’€ / Tired ğŸ˜´ / Neutral ğŸ˜ / Happy ğŸ˜Š
- **EnergÃ­a**: Barra visual + porcentaje
- **Comida Consumida**: Contador
- **Leche Bebida**: Contador
- **Banderas Alcanzadas**: Misiones completadas
- **Saltos**: Total realizados
- **Experiencia**: Decisiones aprendidas
- **Aprendizaje %**: QuÃ© tan convergida (100% = no explora)

## ğŸ—ºï¸ MAPAS PREDEFINIDOS

| Mapa | Complejidad | CaracterÃ­sticas |
|------|-------------|-----------------|
| VacÃ­o | â­ | Nada, exploraciÃ³n pura |
| Simple | â­â­ | 1 bloque + 1 comida |
| ObstÃ¡culos | â­â­â­ | 2 bloques + pincho |
| Laberinto | â­â­â­â­ | NavegaciÃ³n compleja |

## ğŸ® CÃ“MO USAR

### Crear Items
1. **Click Izquierdo**: Comida ğŸ (por defecto)
2. **Rueda del RatÃ³n**: Cambiar tipo de item
3. **Botones**: Seleccionar rÃ¡pidamente

### Cambiar Objetivo
- Selecciona en **"Objetivo del Agente"**
- La IA reinicia su aprendizaje
- Observa cÃ³mo cambia el comportamiento

### ConfiguraciÃ³n
- **Color**: Personaliza el agente
- **Castigo por Saltar**: 
  - ON = saltar cuesta 3 energÃ­a
  - OFF = saltar es gratis
  - La IA lo aprenderÃ¡ automÃ¡ticamente

## ğŸ”¬ EXPERIMENTOS PROPUESTOS

### Experimento 1: Detectar Costo del Salto
```
1. Activa "Castigo por Saltar"
2. Crea 3 comidas en lÃ­nea horizontal
3. Frames 0-50: El agente explora, salta mucho
4. Frames 50-100: Empieza a notar el costo
5. Frames 100+: Reduce saltos drÃ¡sticamente
â†’ Observa "Costo Salto" en debug aumentar
```

### Experimento 2: EvasiÃ³n de Pinchos
```
1. Mapas â†’ ObstÃ¡culos (contiene pincho)
2. Crea mÃ¡s comida alrededor del pincho
3. La IA aprenderÃ¡ a rodearla, NUNCA atravesarla
4. Incluso saltarÃ¡ preemptivamente para evitar
â†’ Es aprendizaje de verdad, no cÃ³digo hardcoded
```

### Experimento 3: Cambiar Objetivos
```
1. Objetivo: Comida (10 min)
2. Objetivo: Leche (cambia instantÃ¡neamente)
3. Observa cÃ³mo adapta su comportamiento
4. Objetivo: Bandera en lugar lejano
â†’ La IA debe navegar, no solo buscar
```

### Experimento 4: NavegaciÃ³n Compleja
```
1. Mapas â†’ Laberinto
2. Objetivo: Alcanzar Bandera
3. Deja corriendo 10 minutos
4. VerÃ¡s mejora DRAMÃTICA en navegaciÃ³n
â†’ Aprendizaje profundo en acciÃ³n
```

## ğŸ“ˆ SEÃ‘ALES DE APRENDIZAJE REAL

âœ… **El agente estÃ¡ aprendiendo:**
- Deja de spammear acciones despuÃ©s de 1-2 minutos
- Busca deliberadamente objetivos
- Evita pinchos con propÃ³sito (no random)
- Reduce saltos si detecta costo
- % de aprendizaje aumenta continuamente

âŒ **Algo estÃ¡ mal:**
- Sigue spammeando despuÃ©s de 3 minutos
- Ignora pinchos constantemente
- Nunca alcanza objetivos
- La grÃ¡fica de aprendizaje no cambia

## ğŸ“ CONCEPTOS TÃ‰CNICOS

### Q-Table (Tabla de Valores)
Cada estado discreto tiene 4 valores Q:

```javascript
Estado: "izquierda_cercano_energÃ­a-baja_suelo_peligro"
{
  left:  -2.5,  // Ir izquierda es malo aquÃ­
  right: -5.0,  // Ir derecha es peor
  jump:  0.8,   // Saltar es bueno (evita peligro)
  idle:  -0.1   // Esperar es neutral
}
```

### Estados Discretos (AutomÃ¡ticos)
Se crean al vuelo basÃ¡ndose en:
- **DirecciÃ³n**: left / right / searching
- **Distancia**: close (< 50) / medium (< 150) / far
- **EnergÃ­a**: low (< 30) / medium (< 70) / high
- **PosiciÃ³n**: ground / air
- **Peligro**: safe / danger

Esto genera ~100 estados Ãºnicos automÃ¡ticamente.

### Recompensas (DiseÃ±o Inteligente)
```javascript
// Cada frame:
-0.03            // Costo base de existir
-0.1 si energÃ­a < 30%  // Incentiva buscar comida

// Al comer:
+50              // Comida normal
+150             // BONUS si objetivo es "food"

+40              // Leche
+140             // BONUS si objetivo es "milk"

+200             // Bandera
+400             // BONUS si objetivo es "flag"

-20              // DaÃ±o de pincho (enseÃ±a a evitar)
```

## ğŸ’¡ OPTIMIZACIONES IMPLEMENTADAS

1. **Sin Spam**: Cooldown de 3 frames entre acciones
2. **DetecciÃ³n Temprana**: Escanea peligro con anticipaciÃ³n
3. **Aprendizaje de Costos**: Promedia Ãºltimos 50 saltos
4. **Estados Comprimidos**: DiscretizaciÃ³n inteligente
5. **Ã‰psilon Decay**: Reduce exploraciÃ³n gradualmente (0.9995/frame)
6. **Prioridades**: Pinchos > Costo > Aleatorio > Aprendido

## ğŸ“š REFERENCIAS

- **Q-Learning Wikipedia**: https://en.wikipedia.org/wiki/Q-learning
- **DeepMind RL Glossary**: https://www.deepmind.com/learning-resources
- **Epsilon-Greedy**: https://en.wikipedia.org/wiki/Multi-armed_bandit

## ğŸ› TROUBLESHOOTING

**P: Â¿Por quÃ© no aprende?**
- âœ“ Verifica que haya items en el mapa
- âœ“ El objetivo debe coincidir con los items
- âœ“ Espera 3+ minutos
- âœ“ Usa mapas simples primero

**P: Â¿Por quÃ© hace spam de movimiento?**
- Normal en primeros 30 segundos (exploraciÃ³n)
- DespuÃ©s deberÃ­a calmarse

**P: Â¿CÃ³mo acelero el aprendizaje?**
- Usa mapas simples
- Pon muchos items
- Desactiva "Castigo por Saltar"

**P: Â¿QuÃ© es "Aprendizaje 85%"?**
- Significa que epsilon es 0.15 (15% exploraciÃ³n)
- ConfÃ­a 85% en lo aprendido
- 100% = no explora mÃ¡s

## ğŸš€ VERSIÃ“N

- **VersiÃ³n**: 2.0 Avanzada
- **Fecha**: Febrero 2026
- **Motor**: Canvas + Vanilla JS
- **Algoritmo**: Q-Learning tabular
- **Estado**: Completamente funcional

---

**Creado por:** Dardo GarcÃ­a + GitHub Copilot  
**Licencia:** MIT  
**Mantenimiento:** Activo
