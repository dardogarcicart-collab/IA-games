# ğŸ¤– Simulador de Vida Artificial con IA

Un simulador interactivo donde una IA aprende mediante **Q-Learning** en tiempo real. El agente comienza sin saber nada y gradualmente descubre quÃ© acciones son beneficiosas.

## âœ¨ CaracterÃ­sticas Principales

### ğŸ§  Inteligencia Artificial Modular

El agente tiene **3 sistemas integrados** que funcionan conjuntamente:

1. **Brain (Cognitivo)** - `js/brain/CognitiveSystem.js`
   - PercepciÃ³n del mundo (visiÃ³n, propiocepciÃ³n, interocepciÃ³n)
   - Procesamiento de emociones y hormonas
   - PredicciÃ³n mental de acciones
   - Toma de decisiones inteligente

2. **Learning (Aprendizaje)** - `js/learning/LearningSystem.js`
   - Q-Learning con tabla Q dinÃ¡mica
   - Descubrimiento de estados
   - Experiencias y patrones aprendidos
   - Epsilon-greedy para exploraciÃ³n/explotaciÃ³n

3. **Physics (FÃ­sica)** - `js/physics/PhysicsEngine.js`
   - Motor fÃ­sico realista con gravedad
   - BiomecÃ¡nica: fatiga muscular, Ã¡cido lÃ¡ctico, oxÃ­geno
   - Sistema metabÃ³lico con consumo de energÃ­a
   - Colisiones precisas (AABB)

### ğŸ“ Aprendizaje Real

- âœ… Comienza **completamente ignorante** (movimiento aleatorio)
- âœ… Aprende mediante **recompensas** al encontrar comida
- âœ… Descubre que ciertos patrones funcionan mejor
- âœ… Gradualmente **optimiza su comportamiento**
- âœ… Tabla Q converge hacia polÃ­tica Ã³ptima

### ğŸ˜Š Estados Emocionales DinÃ¡micos

- **ğŸ˜ Neutral**: Estado base, relajado
- **ğŸ˜  Enojado**: Frustrado por fallos repetidos o baja energÃ­a
- **ğŸ˜´ Cansado**: EnergÃ­a crÃ­tica, necesita comida urgentemente

### ğŸ® Interactividad Studio Real

- **Click Izquierdo**: Crear comida (manzanas rojas)
- **Click Derecho**: Colocar bloques (obstÃ¡culos)
- **Color Customizable**: Elige el color del agente
- **Toggle Castigo**: Activa/desactiva penalizaciÃ³n por saltar
- **Reset**: Reinicia la simulaciÃ³n

### ğŸ“Š EstadÃ­sticas en Tiempo Real

- EnergÃ­a con barra de color (verde/naranja/rojo)
- Estado emocional visible
- Comida consumida
- Saltos realizados
- Experiencias aprendidas

## ğŸš€ CÃ³mo Usar

### 1. Abrir el simulador
```bash
# Navegar a la carpeta
cd /workspaces/IA-games

# OpciÃ³n 1: Abrir IASistem.html directamente en navegador
# OpciÃ³n 2: Usar servidor web
python3 -m http.server 8000
# Luego: http://localhost:8000/IASistem.html
```

### 2. Experimentar

**Observa cÃ³mo aprende:**
1. Al inicio, el agente se mueve aleatoriamente
2. Crea comida haciendo click izquierdo
3. VerÃ¡s que eventualmente busca la comida
4. DespuÃ©s de varias recompensas, aprende el patrÃ³n

**Prueba variables:**
- Desactiva "Castigo por Saltar" y observa mÃ¡s saltos
- ActÃ­valo de nuevo para ver cÃ³mo aprende a evitar saltos innecesarios
- Crea obstÃ¡culos y observa cÃ³mo se adapta

## ğŸ“ Estructura del Proyecto

```
IA-games/
â”œâ”€â”€ IASistem.html              # Archivo principal (HTML + UI)
â”œâ”€â”€ ARQUITECTURA.md            # DocumentaciÃ³n tÃ©cnica detallada
â”œâ”€â”€ README.md                  # Este archivo
â””â”€â”€ js/
    â”œâ”€â”€ brain/
    â”‚   â””â”€â”€ CognitiveSystem.js # Inteligencia y toma de decisiones
    â”œâ”€â”€ learning/
    â”‚   â””â”€â”€ LearningSystem.js  # Q-Learning y memoria
    â”œâ”€â”€ physics/
    â”‚   â””â”€â”€ PhysicsEngine.js   # FÃ­sica y biomecÃ¡nica
    â””â”€â”€ core/
        â””â”€â”€ Entities.js        # Clases: Agent, Food, Block
```

## ğŸ”¬ Variables Realistas Incluidas

### BiomecÃ¡nicas
- Fatiga muscular (afecta altura de saltos)
- Ãcido lÃ¡ctico (acumula con movimiento intenso)
- Deuda de oxÃ­geno (recuperaciÃ³n gradual)
- Temperatura muscular (sube con esfuerzo)
- Flexibilidad de articulaciones

### Sistema Hormonal
- **Adrenalina**: Responde a peligro/urgencia
- **Dopamina**: MotivaciÃ³n y placer por recompensas
- **Cortisol**: EstrÃ©s y ajustes de comportamiento
- **Grelina**: Hambre e impulso de comer
- **Serotonina**: Bienestar general

### Metabolismo
- Tasa basal: 1.2 energÃ­a/frame
- Movimiento: 2.5x multiplicador
- Salto: 15x multiplicador
- Fatiga aumenta consumo hasta 50%

## ğŸ§  CÃ³mo Funciona el Q-Learning

### FÃ³rmula EstÃ¡ndar
```
Q(s,a) = Q(s,a) + Î±[r + Î³ max(Q(s',a')) - Q(s,a)]
```

Donde:
- `s` = estado actual
- `a` = acciÃ³n
- `r` = recompensa
- `Î³` = factor de descuento (0.95)
- `Î±` = velocidad de aprendizaje (0.15)

### ParÃ¡metros
- **ExploraciÃ³n inicial**: 40%
- **Decay de epsilon**: 0.9998 por frame
- **MÃ­nimo de exploraciÃ³n**: 5%

### Proceso

1. **PercepciÃ³n**: Crea estado discreto (ej: `"left_near_high_ground"`)
2. **DecisiÃ³n**: Elige acciÃ³n (greedy o aleatoria)
3. **Recompensa**: Calcula r segÃºn resultado
4. **ActualizaciÃ³n**: Ajusta Q-value de esa acciÃ³n
5. **RepeticiÃ³n**: Convergencia gradual a polÃ­tica Ã³ptima

## ğŸ“ˆ Observables de Aprendizaje

Mira estos indicadores para ver el progreso:

1. **Experiencia (Contador)**: Sube constantemente = aprendiendo
2. **Comida Consumida**: Aumenta = mejora su bÃºsqueda
3. **EnergÃ­a**: Se mantiene estable = decisiones eficientes
4. **ExpresiÃ³n Facial**: Menos enojado = menos frustraciÃ³n
5. **Movimiento**: MÃ¡s dirigido hacia comida = aprendizaje convergido

## ğŸ¯ Recompensas

| Evento | Recompensa |
|--------|-----------|
| Comer comida | +50 (+bonus si energÃ­a baja) |
| Sobrevivir | -0.05 (penalizaciÃ³n pasiva) |
| EnergÃ­a baja | -0.10 (incentiva buscar comida) |
| Salto fallido | -1 (castigo por error) |
| Saltar (si castigo activo) | -2 a -3 |

## ğŸ’¡ Consejos de Uso

### Para Ver Aprendizaje RÃ¡pido
1. Crea muchas manzanas (click izquierdo)
2. ColÃ³calas en lugares variados
3. Observa cÃ³mo el patrÃ³n de bÃºsqueda mejora
4. Nota cÃ³mo baja la exploraciÃ³n aleatoria

### Para Estudiar Comportamiento
1. Desactiva castigo por saltar
2. Crea obstÃ¡culos que requieren saltos
3. Observa cÃ³mo aprende a saltarlos estratÃ©gicamente
4. Vuelve a activar castigo y ve cÃ³mo cambia

### Para Entrenar Larga SesiÃ³n
1. Crea 5-10 manzanas
2. Deja que corra 5-10 minutos
3. Observa convergencia de tabla Q
4. Reset y repite con diferentes configuraciones

## ğŸ”§ PersonalizaciÃ³n

Puedes editar los parÃ¡metros en cada mÃ³dulo:

**En `js/learning/LearningSystem.js`:**
```javascript
this.learningRate = 0.15;       // QuÃ© tan rÃ¡pido aprende
this.discountFactor = 0.95;     // Importancia del futuro
this.epsilon = 0.4;              // ExploraciÃ³n inicial
this.epsilonDecay = 0.9998;     // Velocidad de aprendizaje
```

**En `js/physics/PhysicsEngine.js`:**
```javascript
this.gravity = 0.6;              // Intensidad de gravedad
this.jumpPower = -12;            // Fuerza de salto
this.metabolism.basalMetabolicRate = 1.2;  // Consumo en reposo
```

**En `js/brain/CognitiveSystem.js`:**
```javascript
// Ajustar umbrales de decisiÃ³n
// Modificar pesos de sensores
// Cambiar criterios emocionales
```

## ğŸ“š Para Entender Mejor

Revisa `ARQUITECTURA.md` para:
- DocumentaciÃ³n tÃ©cnica completa
- Flujo de ejecuciÃ³n por frame
- Variables disponibles
- API de cada mÃ³dulo
- Ejemplos de uso avanzado

## ğŸ¨ VisualizaciÃ³n

**Elementos visuales:**
- ğŸŸ¥ Agente (cuadrado coloreable)
- ğŸ Comida (manzana roja pulsante)
- ğŸ§± Bloques (ladrillos marrones)
- ğŸŸ© Suelo con cÃ©sped decorativo
- ğŸŒ¤ï¸ Cielo con gradiente azul

**Expresiones faciales:**
- Ojos y boca que cambian segÃºn estado emocional
- Parpadeo de estrÃ©s cuando energÃ­a muy baja
- Animaciones suaves

## ğŸ› Troubleshooting

**Si no aparece nada:**
- Verifica que los archivos JS estÃ©n en `js/brain/`, `js/learning/`, etc.
- Abre consola (F12) para ver errores
- Recarga la pÃ¡gina

**Si el agente no se mueve:**
- Crea comida (click izquierdo)
- Espera un momento (necesita explorar primero)
- Verifica que no haya errores en consola

**Si estÃ¡ muy lento:**
- Son demasiados bloques y comida
- Haz reset y comienza con menos objetos
- La fÃ­sica y IA corren en el mismo thread

## ğŸ“ Licencia

CÃ³digo libre para experimentar y aprender sobre IA.

## ğŸš€ Mejoras Futuras Posibles

- [ ] VisualizaciÃ³n de tabla Q en tiempo real
- [ ] GrÃ¡ficas de convergencia
- [ ] Guardado/carga de tabla Q entrenada
- [ ] MÃºltiples agentes aprendiendo simultÃ¡neamente
- [ ] Entorno mÃ¡s complejo con mÃ¡s tipos de objetos
- [ ] Red neuronal en lugar de Q-Learning discreto
- [ ] ExportaciÃ³n de datos de entrenamiento

---

**Â¡DiviÃ©rtete observando cÃ³mo una IA aprende desde cero!** ğŸ¤–âœ¨
